# Audio Transcription MCP - Environment Configuration
# Copy this file to .env and fill in your values

# =============================================================================
# REQUIRED: Hugging Face Token
# =============================================================================
# Get your token at: https://huggingface.co/settings/tokens
# You must accept model terms at:
#   - https://huggingface.co/pyannote/speaker-diarization-3.1
#   - https://huggingface.co/pyannote/segmentation-3.0
HF_TOKEN=hf_your_token_here

# =============================================================================
# OPTIONAL: Whisper Model Configuration
# =============================================================================
# Model size options: tiny, base, small, medium, large-v3
# Larger models = better accuracy but slower and more memory
WHISPER_MODEL=large-v3

# Language for transcription (ISO 639-1 code)
# Default: en (English)
LANGUAGE=en

# =============================================================================
# OPTIONAL: File Processing
# =============================================================================
# Maximum file size in GB (default: 1)
MAX_FILE_SIZE_GB=1

# Output directory for transcriptions (default: ./output)
OUTPUT_DIR=./output

# Input directory for audio files (default: ./input)
INPUT_DIR=./input

# =============================================================================
# OPTIONAL: MCP Server Configuration
# =============================================================================
# Transport mode: stdio (local) or streamable-http (Docker)
MCP_TRANSPORT=streamable-http

# HTTP port (only used when MCP_TRANSPORT=streamable-http)
MCP_PORT=8080

# =============================================================================
# OPTIONAL: GPU Configuration (NVIDIA only)
# =============================================================================
# CUDA device to use (default: 0)
# Set to -1 to force CPU mode
CUDA_VISIBLE_DEVICES=0
